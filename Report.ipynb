{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report - by Kevin Kosacoff\n",
    "## Gather\n",
    "\n",
    "We first started the gathering process by downloading the provided csv file: `twitter-archive-enhanced.csv`. This was pretty straightforward as it then needed to be uploaded manually, and then read with Pandas.\n",
    "    \n",
    "For the 2nd file, `image_predictions.tsv`, we needed to download it programatically. This was done with the Requests package, where we indicated an url for which we wanted to request its content. Once we had the content we need to save this content in a file locally for future usage. We do that creating the file `image_predictions.tsv` in our directory, and then writing the content of the url with the `file.write` function. We then open the file and save it to a pandas dataframe using the `/t` separator as it is a tsv file.\n",
    "\n",
    "The 3rd file was the most difficult file to gather as we needed to make get the information directly from Twitter. We did this via an API that facilitates communication with Twitter, the `tweepy` package. We 1st need to create a developer account with Twitter so that we can get an unique user and key to access the tweets information. We pass that information to some variables and create the api object of the tweepy package. Now we can query tweets information easily. For this file I decided to query each tweet_id of the `twitter-archive-enhanced.csv` file, and the save the Fav and RT count for each of those tweets. This had to be saved in a JSON format and then saved locally in `.txt` for future usage. The script took about 15 minutes to run as it had to query each tweet individually and had to take breaks as it reached a querying limit. Then I created a dictionary reading line by line of the `.txt` file and then passed that dictionary to a pandas dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess\n",
    "\n",
    "For the assessment part I started with the visual assessment. For that I just printed each of the data frames and scrolled and looked for things that didn't make sense, like for example I saw the twitter enhance archive file dog names that were `such, a or an` and then looking at the original tweet I realized that it wasn't the dog's real name. Then I moved on to the programatic assessment part where I used the `.info()` method on the dataframes to see more information about them. With this I was able to find that the timestamp column was in string format which was wrong as we wouldn't be able to do any time analysis with that format. I also found that there where some rows that contained RT and replies information, which was not okay according to the project details information. For the tidyiness assessment I used a combination of both, for example for the dog stages I inspected visually what was on each of its columns and if it made sense to had 4 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean\n",
    "\n",
    "For each of the assessments from the 2nd part, I defined a course of action, code it and finally test if it worked. The one I had more difficulties with was with the tidyiness issue of the 4 column stages of the dog. I had to use the pandas melt function but because of having a lot of None values, the function didn't work inmediatly. Because of that I performed 2 melts into 2 sepparate dataframes, one that contained at least one stage, and the other were it did not have any stage. I then stacked both data frames and then inspected to see if it was okay, and it was."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
